# -*- coding: utf-8 -*-
"""FraudDetection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gBByid9NWLbibJKkGPyGLJs_JDB33IUP

# **Fraud Detection**

# Project By : Harsha Siddaganagaiah
"""



"""#**Load, Structure of Data and Summary Statistics**

**Loaded the transactions data in line-delimited JSON format**
"""

import requests

"""**Mounted the data through Google Drive in Google Colab for easy access as data is greater than 600 MBs**"""

from google.colab import drive
drive.mount('/content/drive')

"""**Loaded JSON format into Pandas Data Frames which are just like SQL or Excel Tables**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df = pd.read_json('/content/drive/MyDrive/FraudDetection/transactions.txt', lines=True)

"""**Necessary Python Packages and Libraries are loaded for further analysis**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import datetime
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder
from imblearn.under_sampling import RandomUnderSampler
from sklearn.model_selection import train_test_split
!pip install imblearn
import seaborn as sns
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier

"""#**Structure of the data**

**Displayed Both Top and Bottom rows of the data to have a better look on the structure and attributes before doing any data analysis**
"""

df.head(5)

df.tail(5)

"""**Some Basic Information about the dataset which includes the Type of columns it has, number of columns and total number of records**"""

df.info()

df.columns

len(df)

len(df.columns)

"""**Shape represents the Dimensions like a Matrix**

**Number of Records = 786363**

**Number of Attributes/Columns = 29**
"""

df.shape

"""**Describe helps in Analyzing the data at a glance** """

df.describe()

"""**Checking for Null values in data, because either these are filled with an appropriate value with respect to each column or completely removed from the dataset, depending on a given use case**"""

df.replace('', np.nan, inplace=True)
df.isnull().sum()

"""**Unique Values in each attribute also helps explore the structure of the dataset**"""

df.nunique()

"""

**Here that most important attribute is "isFraud" which has 1.6% True Values**"""

df['isFraud'].value_counts()

colsToObserve = df.columns[(df.nunique() <= 10)]

"""**When an attribute has less number of Unique Values it is always a good criteria to look at them keenly to know more about the structure as a whole and how the other attributes are behaving around these small buckets**"""

for col in colsToObserve:
  if len(df[col].unique())>1:
    print(col + " : ")
    print(df[col].unique())

"""# **Data Visualization**

**Plots, Histograms, Box Plots, Correlation Matrix and Hypothesis about the Structure of Data, Deep Diving in Fraudulent Aspects to find correlation and causation**

**Converting True/False Booleans as 0s and 1s**
"""

df[["isFraud", "cardPresent", "expirationDateKeyInMatch", ]] *= 1

"""**Buckets/Unique Values in Credit Card Limit are shown below:**

**Most of the Credit Cards have 5000 as their limit, on second number it is 15000. Interestingly, credit cards with 50,000 limit also exists in the dataset**
"""

import seaborn as sns



e=pd.DataFrame(df.creditLimit.value_counts()).reset_index(drop=False)
e.columns=['creditLimit','Frequency'] # rename columns
plt.figure(figsize=(14,6)) # adjust figure size
ax = sns.barplot(x="creditLimit", y="Frequency", data=e) # draw barplot

"""**4 Types of Merchant Country Code exists in the datset where US is a dominant entity as shown**"""

e=pd.DataFrame(df.merchantCountryCode.value_counts()).reset_index(drop=False)
e.columns=['merchantCountryCode','Frequency'] # rename columns
plt.figure(figsize=(14,6)) # adjust figure size
ax = sns.barplot(x="Frequency", y="merchantCountryCode", data=e) # draw barplot

"""**5 POS Entery Mode exists where "05", "09", and "02" covers most of the values**"""

e=pd.DataFrame(df.posEntryMode.value_counts()).reset_index(drop=False)
e.columns=['posEntryMode','Frequency'] # rename columns
plt.figure(figsize=(14,6)) # adjust figure size
ax = sns.barplot(x="Frequency", y="posEntryMode", data=e) # draw barplot

"""**3 POS Condition Mode exists where "01" is dominant among all**"""

e=pd.DataFrame(df.posConditionCode.value_counts()).reset_index(drop=False)
e.columns=['posConditionCode','Frequency'] # rename columns
plt.figure(figsize=(14,6)) # adjust figure size
ax = sns.barplot(x="Frequency", y="posConditionCode", data=e) # draw barplot

"""
**There are 3 transaction types:**

**1.Purchase**

**2.Reversal**

**3.Address Verification**

**where most values lie inside Purchase Type.**



"""

e=pd.DataFrame(df.transactionType.value_counts()).reset_index(drop=False)
e.columns=['transactionType','Frequency'] # rename columns
plt.figure(figsize=(14,6)) # adjust figure size
ax = sns.barplot(x="transactionType", y="Frequency", data=e) # draw barplot

"""**If card is Present during the transaction also plays an important role and dataset is fairly distributed among the two values as True/False**"""

e=pd.DataFrame(df.cardPresent.value_counts()).reset_index(drop=False)
e.columns=['cardPresent','Frequency'] # rename columns
plt.figure(figsize=(14,6)) # adjust figure size
ax = sns.barplot(x="cardPresent", y="Frequency", data=e) # draw barplot



"""**Expiration Date Key In Match is highly skewed to 0 value**"""

e=pd.DataFrame(df.expirationDateKeyInMatch.value_counts()).reset_index(drop=False)
e.columns=['expirationDateKeyInMatch','Frequency'] # rename columns
plt.figure(figsize=(14,6)) # adjust figure size
ax = sns.barplot(x="expirationDateKeyInMatch", y="Frequency", data=e) # draw barplot

df.expirationDateKeyInMatch.value_counts()

"""**isFraud as described earlier is clearly an example of IMBALANCED DATSET with respect to this attribute. But It is expected to be like that in real world scenario as Frauds are always less than 5% with respect to whole dataset. Here it is 1.6%**"""

e=pd.DataFrame(df.isFraud.value_counts()).reset_index(drop=False)
e.columns=['isFraud','Frequency'] # rename columns
plt.figure(figsize=(14,6)) # adjust figure size
ax = sns.barplot(x="isFraud", y="Frequency", data=e) # draw barplot

"""**Gouping By with respect to Customer ID represents how many Unique Customers exist in the dataset which are 5000**"""

df.groupby(['customerId']).count()

"""**Account number is similar to Customer ID and represents 5000 unique entries**"""

df.groupby(['accountNumber']).count()

"""**To know more about the transaction data set it is important to know the Top Merchants where most of the Transactions happen.**

**Here we can clearly see which are the Top Merchants:**

1.Uber

2.Lyft

3.oldnavy.com

4.staples.com

5.alibaba.com

6.apple.com

7.walmart.com

8.cheapfast.com

9.ebay.com

10.target.com

11.amazon.com
"""

important_merchants = df['merchantName'].value_counts()
important_merchants = important_merchants.loc[important_merchants.values > 10000]
important_merchants

"""**Here we can clearly see that most transactions happen at:**


*   Online Retail

*   Food

*   Entertainment
*   Online Gifts


*   Ride Share


*   Hotels

"""

t=pd.DataFrame(df['merchantCategoryCode'].value_counts()).reset_index(drop=False)
t.columns=['MerchantCategory','Count'] 
plt.figure(figsize=(24,6)) 
ax = sns.barplot(x="MerchantCategory", y="Count", data=t)

"""**Let's explore Numerical Valued Attributes and in Credit Card Transactions these attributes hold high importance**"""

df_num = df[['creditLimit', 'availableMoney', 'transactionAmount', 'currentBalance' ]]

df_num

"""**Separatelty describing these numerical attributes to get the essence of data and its distribution.**

**Here we can see Available Money can also go Negative, which is -1005.63 as minimum value**
"""

df_num.describe()

"""**Most of the Credit limits are under 10,000.**

**But some goes till 20,000 and 50,000 as well.**

**While data Modelling this information can be really useful**
"""

plt.hist(df['creditLimit'], bins = 30)
plt.ylabel('Frequency', fontsize = 16)
plt.xlabel('credit Limit', fontsize = 16)

"""**Available Money mostly lie under the buckets of less than 10,000 and is Right Skewed, which actually makes sense**"""

plt.hist(df['availableMoney'], bins = 30)
plt.ylabel('Frequency', fontsize = 16)
plt.xlabel('available Money', fontsize = 16)

"""**Interestingly Transaction amount mostly falls under 500 and is Right Skewed**"""

plt.hist(df['transactionAmount'], bins = 30)
plt.ylabel('Frequency', fontsize = 16)
plt.xlabel('transaction Amount', fontsize = 16)

"""**The Current Balance is mostly under 10,000, it is also Right Skewed**"""

plt.hist(df['currentBalance'], bins = 20)
plt.ylabel('Frequency', fontsize = 16)
plt.xlabel('current Balance', fontsize = 16)

"""**Box Plots help analyzing the Outliers in the datasets and the skewness**

**It shows how these Numerical Atrributes are Right Skewed**
"""

sns.boxplot(data=df_num['creditLimit'])

sns.boxplot(data=df_num['availableMoney'])

sns.boxplot(data=df_num['transactionAmount'])

sns.boxplot(data=df_num['currentBalance'])

merchant_with_fraud = df.loc[df['isFraud'] == 1, ['merchantCategoryCode']]
merchant_with_fraud

"""**The graph below shows those Merchants where most of the fraudulent transactions happen.**

**Top 3 are:**


*   **Online Retail**

*   **Online Gifts**
*   **Ride Share**


"""

e=pd.DataFrame(merchant_with_fraud.merchantCategoryCode.value_counts()).reset_index(drop=False)
e.columns=['merchantCategoryCode','Frequency'] # rename columns
print("Frauds Happening mostly according to this distribution: ")
plt.figure(figsize=(14,6)) # adjust figure size
ax = sns.barplot(x="merchantCategoryCode", y="Frequency", data=e) # draw barplot

"""**The Merchant Names where most Fraud happens:**

**Top 5 are:**



1.   **Lyft**
2.   **ebay.com**

1.   **Fresh Flowers**
2.   **Uber**

1.   **walmart.com**

"""

merchant_name__with_fraud = df.loc[df['isFraud'] == 1, ['merchantName']]

important_merchants = merchant_name__with_fraud['merchantName'].value_counts()
important_merchants = important_merchants.loc[important_merchants.values > 300]
print("Most Frauds Happening at these merchants:")
important_merchants

"""**Fortunately, The account IDs which get most frauds can also be seen through dataset.**

**Top 3 Account IDs are:**


1.   **380680241**
2.   **782081187**

1.   **246251253**

"""

accountNumber_with_fraud = df.loc[df['isFraud'] == 1, ['accountNumber']]

acc_fraud = accountNumber_with_fraud['accountNumber'].value_counts()
acc_fraud = acc_fraud.loc[acc_fraud.values > 200]
print("Most Frauds Happening on these accounts:")
acc_fraud

"""**Account IDs and Customer IDs are the same.**"""

customers_with_fraud = df.loc[df['isFraud'] == 1, ['customerId']]

cust_fraud = customers_with_fraud['customerId'].value_counts()
cust_fraud = cust_fraud.loc[cust_fraud.values > 200]
print("Most Frauds Happening with these customer Ids:")
cust_fraud

"""**Correlation Matrix between Numerical Attributes also tell important information about the given process.**"""

import pandas as pd
import numpy as np
corr = df_num.corr()
corr.style.background_gradient(cmap='coolwarm').set_precision(2)

"""# **Data Wrangling**

**Till know I didn't remove any empty Columns Now removing 6 empty columns.**
"""

df.drop(['echoBuffer','merchantCity','merchantState','merchantZip','posOnPremises','recurringAuthInd'],
                  axis=1,inplace=True)
df

df.head(5)

df_2 = df.copy()

"""**The assumption is Duplicate Transactions are any record other than Address Verification because it has Transaction Amount = 0**"""

df_2 = df_2[df_2.transactionType != "ADDRESS_VERIFICATION"]
df_2

"""**Adding Date Column in the Data Frame to computer Results independent of just Date-Time**"""

df_2['transactionDate'] = pd.to_datetime(df_2['transactionDateTime']).dt.date
df_2['transactionDate']

"""**Here I am taking those attributes which can uniquely identify and help in indentifying a duplicate transaction at a given point:**

**The attributes used are:**

*   **accountNumber (It will be unique for that customer)**

*   **transactionAmount (the amount needs to be check for duplicated occurances)**

*   **merchantName (where the transaction is actually taking place)**

*   **acqCountry (the country where it can take place)**
*   **accountOpenDate (account open date will be same)**


*   **merchantCategoryCode ( merchantName can come as an empty index, then this merchantCategoryCode will be most useful in that scenario)**

*   **cardLast4Digits (always help in analyzing duplicate tansactions)**


"""

duplicate_trans = df_2[df_2.duplicated(['accountNumber', 'transactionAmount','merchantName', 'acqCountry' , 'accountOpenDate' ,'merchantCategoryCode','cardLast4Digits' ], keep = False)]
duplicate_trans

duplicate_trans.shape

"""**Frauds in Duplicated Transaction can expected to be more.**

**The Top 3 Merchants where duplicated transactions happened and Frauds took place are:**




1.   **Fresh Flowers**
2.   **Lyft**

1.   **ebay.com**






"""

duplicate_trans_fraud = duplicate_trans.loc[duplicate_trans['isFraud'] == 1, ['merchantName']]

dup_fraud = duplicate_trans_fraud['merchantName'].value_counts()
dup_fraud = dup_fraud.loc[dup_fraud.values >= 30]
print("Most Frauds Happening within duplicates are at these places:")
dup_fraud

"""**Reversal Transactions can be found directly either by using the attribute Transaction Type.**

**That comes out to be Reversal = 20303**

**Transaction Amount of Reversal Transaction = 2821792.5**
"""

len(df_2.loc[df_2['transactionType'] == 'REVERSAL'])

np.sum(df_2.loc[df_2['transactionType'] == 'REVERSAL'].transactionAmount)

"""**Merchnats where Frauds took place according to the given attribute transaction type = Reversal in the original dataset:**



1.   **Lyft**
2.   **walmart.com**

1.   **gap.com**
"""

df_2_rev = df_2.loc[df_2['isFraud'] == 1, ['merchantName','transactionType']]

df_2_rev = df_2_rev.loc[df_2_rev['transactionType'] == 'REVERSAL', ['merchantName']]

df_2_rev_fraud = df_2_rev['merchantName'].value_counts()
df_2_rev_fraud = df_2_rev_fraud.loc[df_2_rev_fraud.values >= 10]
print("Most Frauds Happening within Original data and reversal transaction type are at these places:")
df_2_rev_fraud

"""**Another method to find Reversal Transaction is to look directly into the duplicated transactions:**

**Reversal Transactions in Duplicated records = 17826**

**Transaction Amount for Reversal Transactions in Duplicated records = 2669859.56**
"""

len(duplicate_trans.loc[duplicate_trans['transactionType'] == 'REVERSAL'])

np.sum(duplicate_trans.loc[duplicate_trans['transactionType'] == 'REVERSAL'].transactionAmount)

"""**Most Frauds that happen in Reversal Transactions of Duplicated Subset are for these Merchants:**



1.   **Lyft**

1.   **Fresh Flowers**
2.   **Walmart.com**

"""

duplicate_trans_fraud = duplicate_trans.loc[duplicate_trans['isFraud'] == 1, ['merchantName','transactionType']]

reverse_trans_fraud = duplicate_trans_fraud.loc[duplicate_trans_fraud['transactionType'] == 'REVERSAL', ['merchantName']]

rev_fraud = reverse_trans_fraud['merchantName'].value_counts()
rev_fraud = rev_fraud.loc[rev_fraud.values >= 10]
print("Most Frauds Happening within Duplicate REVERSAL are at these places:")
rev_fraud

def timeToDaTe(t):
  return datetime.datetime.strptime(t, '%Y-%m-%dT%H:%M:%S')

"""**Multi Swipe Transactions**

**These can be find as grouping by the uniquely identifying attributes in such a sorted way, that if the subsequent transactions happen in next 3 minutes, They will be marked as Mutli-Swiped Transactions**
"""

NonRevDup = duplicate_trans[duplicate_trans['transactionType'] != 'REVERSAL']
NonRevDup

import datetime

MultiSwipeTrans = []
for i, g in NonRevDup.groupby(['accountNumber', 'transactionAmount','merchantName', 'acqCountry' , 'accountOpenDate' ,'merchantCategoryCode','cardLast4Digits' ]):
    prev_interval = datetime.datetime.strptime('1000-05-05T22:47:50', '%Y-%m-%dT%H:%M:%S')
    g['transactionDateTime'] = g['transactionDateTime'].apply(timeToDaTe)
    g = g.sort_values(by=['transactionDateTime'])
  
    for r_i, row in g.iterrows():
        time = row['transactionDateTime']
        if time < prev_interval:
            MultiSwipeTrans.append(r_i)
        prev_interval = (time + datetime.timedelta(seconds=180))
MultiSwipeDf = df.iloc[MultiSwipeTrans]

"""**Multi Swipe Transactions come out to be = 7422**

**Total Transaction Amount = 1098489.81**
"""

print("MultiSwipe Transactions: ", len(MultiSwipeDf))
print("Total of transactions amount", np.sum(MultiSwipeDf['transactionAmount']))

"""**The most Frauds happening in the Multi-Swipe Transaction subset are:**

**Top 3:**



1.   **ebay.com**
2.   **sears.com**

1.   **cheapfast.com**
"""

ms_trans_fraud = MultiSwipeDf.loc[MultiSwipeDf['isFraud'] == 1, ['merchantName']]

ms_ext = ms_trans_fraud['merchantName'].value_counts()
ms_ext = ms_ext.loc[ms_ext.values >= 5]
print("Most Frauds Happening within Multi-Swipe Transactions are at these places:")
ms_ext

"""**Fraud Ratio in Duplicated Transactions are: 0.9%**"""

duplicate_trans['isFraud'].value_counts()

"""**Fraud Ratio in Reversal Transactions of Duplicated Data subset = 1.7%**"""

dup_rev = duplicate_trans.loc[duplicate_trans['transactionType'] == 'REVERSAL']
dup_rev['isFraud'].value_counts()

"""**Fraud Ratio in Reversal Transactions of Original Data Set = 1.68%**"""

df2_rev = df_2.loc[df_2['transactionType'] == 'REVERSAL']
df2_rev['isFraud'].value_counts()

"""**Fraud Ratio in Multi-Swipe Transactions = 1.76%**"""

MultiSwipeDf['isFraud'].value_counts()

"""# **Model, Classification, Removing Outliers, Further Cleaning, Feature Encoding, Feature Engineering, Metrics, Evaluation, Results, Further Improvement and Domains**

**Interquartile Ranges are generally used for removing the outliers in a numerical attribute**
"""

def remove_outlier(df_in, col_name):
    q1 = df_in[col_name].quantile(0.25)
    q3 = df_in[col_name].quantile(0.75)
    iqr = q3-q1 #Interquartile range
    fence_low  = q1-1.5*iqr
    fence_high = q3+1.5*iqr
    df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]
    return df_out

df = remove_outlier(df, 'creditLimit' )
df

"""**We can see which buckets are removed from the dataset, as for Models to generalize the dataset well it is advised to remove outliers.**"""

e=pd.DataFrame(df.creditLimit.value_counts()).reset_index(drop=False)
e.columns=['creditLimit','Frequency'] # rename columns
plt.figure(figsize=(14,6)) # adjust figure size
ax = sns.barplot(x="creditLimit", y="Frequency", data=e) # draw barplot

plt.hist(df['creditLimit'], bins = 30)
plt.ylabel('Frequency', fontsize = 16)
plt.xlabel('creditLimit', fontsize = 16)

df = remove_outlier(df, 'availableMoney' )

plt.hist(df['availableMoney'], bins = 30)
plt.ylabel('Frequency', fontsize = 16)
plt.xlabel('availableMoney', fontsize = 16)

df = remove_outlier(df, 'transactionAmount' )

plt.hist(df['transactionAmount'], bins = 30)
plt.ylabel('Frequency', fontsize = 16)
plt.xlabel('transactionAmount', fontsize = 16)

df = remove_outlier(df, 'currentBalance' )

plt.hist(df['currentBalance'], bins = 20)
plt.ylabel('Frequency', fontsize = 16)
plt.xlabel('currentBalance', fontsize = 16)



"""**Now Box Plots are again drawn to help visualize how outliers were impacting the overall skewness**"""

sns.boxplot(data=df['creditLimit'])

sns.boxplot(data=df['availableMoney'])

sns.boxplot(data=df['transactionAmount'])

sns.boxplot(data=df['currentBalance'])

df.shape

"""#**Feature Engineering**

**Making an additional Column derived from the equality of CardCVV and EnteredCVV as shown**
"""

df['CVVMatched'] = df['cardCVV'] == df['enteredCVV']

"""**4 times that represent morning, afternoon/evening, night and late/mid night are also added as feature**"""

def time(t):
  if isinstance(t, str):
    t = datetime.datetime.strptime(t,'%Y-%m-%dT%H:%M:%S').time()

    t1 = datetime.datetime.strptime('05:00:00','%H:%M:%S').time()
    t2 = datetime.datetime.strptime('11:00:00','%H:%M:%S').time()
    t3 = datetime.datetime.strptime('17:00:00','%H:%M:%S').time()
    t4 = datetime.datetime.strptime('23:00:00','%H:%M:%S').time()    

    if t >= t1 and t < t2:
      return 0
    elif t >= t2 and t < t3:
      return 1
    elif t >= t3 and t < t4:
      return 2
    elif t >= t4 or t < t1:
      return 3
      
df['transactionTimeCat'] = df['transactionDateTime'].apply(time)

df['transactionTimeCat'].value_counts()

df['CVVMatched'].value_counts()

"""**The Final Numbers of Columns Selected are 16, I removed the unique identifiers and empty columns.**

**Further Principal Component Analysis can also be tried to reduce the dimensions further but for the given case it is not required.**

**Variance of the dataset is still maintained to be 99%**
"""

df = df.drop(columns=['accountNumber','customerId', 'transactionDateTime', 'transactionDateTime', 'cardLast4Digits', 'cardCVV', 'enteredCVV', 'accountOpenDate', 'dateOfLastAddressChange', 'currentExpDate'])
print('Final total columns: ', len(df.columns))

"""**Transormation from Categorical and Booleans to Numerical**"""

df[["CVVMatched" ]] *= 1

from sklearn.preprocessing import LabelEncoder


le = LabelEncoder()
columns = ['merchantName', 'acqCountry', 'merchantCountryCode', 'merchantCategoryCode', 'transactionType', 'posEntryMode', 'posConditionCode']
for i in columns:
    df[i] = le.fit_transform(df[i])

for i in df.columns:
    df[i] = df[i].fillna(df[i].mode()[0])

"""**As the Data is Imbalanced, so to run classification models efficiently it is advised to use a Down or Upper Sampling technique to balance the dataset for better results**

**Here Metrics are very important:**

*   **For Imblanced Data Sets F1 Score is a good metric.**
*   **Precision and Recall can also be checked as Precision is important in identifying fraud cases precisely because banks do not want to decline a valid transaction.**

*   **Accuracy Measure can only be used after making the dataset balanced, in our modelling that can also come handy**

**Using UnderSampling Technique**
"""

from imblearn.under_sampling import RandomUnderSampler


y = df['isFraud']
X = df.drop(columns = ['isFraud'])
RUS = RandomUnderSampler()
X, y = RUS.fit_resample(X, y)
print(len(X), len(y))

"""**For Model Evaluation Always Train the model on Training dataset and Verify the evaluation metrics on test data sets.**

"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)
print(len(X_train), len(X_test))
print(len(y_train), len(y_test))

"""**Class for Training and Evaluation:**"""

class TrainAndEval:

  def __init__(self, clf, X_train, y_train, X_test, y_test):
    self.clf = clf
    self.X_train = X_train
    self.y_train = y_train
    self.X_test = X_test
    self.y_test = y_test

  def train(self):
    return self.clf.fit(self.X_train, self.y_train)

  def eval(self):
    y_pred = self.clf.predict(self.X_test)
    tn, fp, fn, tp = confusion_matrix(self.y_test,y_pred).ravel()
    print('True Negatives:', tn)
    print('False Positives:', fp)
    print('False Negatives:', fn)
    print('True Positives:', tp)
    print(classification_report(self.y_test,y_pred))

"""**Reason for choosing Classifiers and Particularly Tree Structures:**



*   **Whenever you have a given label to model around, classification is the best choice.**

*   **Tree Based Classifiers are best in terms of interpretation of the end results. Because in real world scenarios, detecting fraud is just one part. But what actions to take to handle and optimize the processes, these tree structures help through the attributes hierarchy that which actions can be taken and some buckets can be made to start with.**

*   **Simple SVM and Logistic regression are always the best baselines to compare-with**

# **Gradient Boosting Classifier:**
"""

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import GradientBoostingClassifier
params = {'n_estimators': [10, 50, 125, 500], 'max_depth': [1, 3, 5], 
          'learning_rate': [0.05, 0.1, 0.5, 0.9]}
grid = t(estimator=GradientBoostingClassifier(random_state = 0), param_grid=params, cv=5, verbose=3)
grid = grid.fit(X_train, y_train)

clf = grid.best_estimator_
GBT = TrainAndEval(clf, X_train, y_train, X_test, y_test)
GBT.eval()

"""**We can see that Gradient Boosting Classifier gave good results:**


*   **F1 Score = 73%**
*   **Precision = 73%**
*   **Recall = 73%**
*   **Accuracy = 72%**

# **XGBoost**
"""

params = {'n_estimators': [10, 50, 125, 500], 'max_depth': [1, 3, 5], 
          'learning_rate': [0.05, 0.1, 0.5,  0.9]
        }

gridXGB = GridSearchCV(estimator=XGBClassifier(), param_grid=params, cv=5, verbose=3)
gridXGB = gridXGB.fit(X_train, y_train)

gridXGB = gridXGB.best_estimator_
XGB = TrainAndEval(gridXGB, X_train, y_train, X_test, y_test)
XGB.eval()

"""**We can see that Grid XGBoost Classifier gave better results than the last one:**


*   **F1 Score = 73%**
*   **Precision = 73%**
*   **Recall = 74%**
*   **Accuracy = 73%**

# **SVM**
"""

params = {
    'C': [0.05, 0.1, 0.5, 1, 10, 50, 100],
    'kernel': ['poly', 'rbf'],
    'degree': [1, 2, 3, 4]
}

gridSVM = GridSearchCV(estimator=SVC(), param_grid=params, cv=5, verbose=3)
gridSVM = gridSVM.fit(X_train, y_train)

gridSVM = gridSVM.best_estimator_
SVM = TrainAndEval(gridSVM, X_train, y_train, X_test, y_test)
SVM.eval()

"""**The Results of Grid SVM Classifier:**


*   **F1 Score = 59%**
*   **Precision = 65%**
*   **Recall = 54%**
*   **Accuracy = 62%**

**The reason Random Forest is used but not simple Decision Tree becasue decision trees tend to over-fit quickly.**

# **Random Forest**
"""

from sklearn.ensemble import RandomForestClassifier
params = {
    'n_estimators': [10, 50, 100, 300, 500],
    'max_depth': [1, 3, 5],
    'bootstrap': [True, False],
    'min_samples_leaf': [1, 2, 4],
    'min_samples_split': [2, 5, 10]
}

gridRF = GridSearchCV(estimator=RandomForestClassifier(), param_grid=params, cv=5, verbose=3)
gridRF = gridRF.fit(X_train, y_train)

gridRF = gridRF.best_estimator_
RF = TrainAndEval(gridRF, X_train, y_train, X_test, y_test)
RF.eval()

"""**The Results of Random Forest Classifier:**


*   **F1 Score = 67%**
*   **Precision = 69%**
*   **Recall = 65%**
*   **Accuracy = 67%**

# **Logistic Regression**
"""

params = {
    'penalty': ['l2', 'none'],
    'C': [0.005, 0.01, 0.1, 0.5, 1, 10],
    'fit_intercept': [True, False],
    'n_jobs': [-1]
}

gridLR = GridSearchCV(estimator=LogisticRegression(max_iter=1000), param_grid=params, cv=5, verbose=3)
gridLR = gridLR.fit(X_train, y_train)

gridLR = gridLR.best_estimator_
LR = TrainAndEval(gridLR, X_train, y_train, X_test, y_test)
LR.eval()

"""**The Results of Logisitc Regression Classifier:**


*   **F1 Score = 63%**
*   **Precision = 66%**
*   **Recall = 61%**
*   **Accuracy = 64%**
"""

XAxis = ['Precision', 'Recall', 'F1-Score', 'Accuracy']
LRValues = [66,61,63,64]
SVMValues = [65, 54, 59, 62]
RFValues = [69, 65, 67, 67]
XGBValues = [73, 74, 73, 73]
GBTValues = [73, 73, 73, 72]

bar_width = 0.15
index = np.arange(4)

fig, ax = plt.subplots()
LR = ax.bar(index,LRValues, width=bar_width, label = "Logistic Regression") 
SVM = ax.bar(index+bar_width,SVMValues, width=bar_width, label = "SVM") 
RF = ax.bar(index+bar_width+bar_width,RFValues, width=bar_width, label = "Random Forest") 
XGB = ax.bar(index+bar_width+bar_width+bar_width,XGBValues, width=bar_width, label = "XGBoost") 
GBT = ax.bar(index+bar_width+bar_width+bar_width+bar_width,GBTValues, width=bar_width, label = "Gradient Boosting Trees") 


ax.set_xlabel('')
ax.set_ylabel('')
ax.set_title('Comparison of 5 models against 4 metrics')
ax.set_xticks(index + bar_width / 2)
ax.set_xticklabels(XAxis)
ax.legend(loc = 'lower right')

fig.set_size_inches(10,6)

plt.show()

fig.savefig('MetricsModels.png',dpi = 200)

"""# **RESULTS**

* **It can clearly be seen that XGBoost performed better than all the other classifiers:**

* **Where F1 Score and Accuracy of XGBoost is highest = 73% and Precision of 73% to detect frauds**

* **Multiple modules of Machine Learning pipeline are used, Data Visualizations, Data Analysis, Data wrangling, Feature engineering, Predictive Modelling, Interpretation of the model, comparing results.** 

* **Various Metrics such as Precision, Recall, Accuracy, F1 Scores and Balanced Accuracy are checked.**

* **Logistic regression is always the best baselines to compare-with. Here we got significant 62\% accuracy.**

# **Further Improvements and Remarks:**

* Clustering can also be done where labels are not required to get other meaningful insights out of data.
  
* In the data set we had clearly seen that there were some Customer IDs which were highly fraudulent. So, a user-profiling based statistical model would significantly improve the results.

* Nothing beats the quality of data. So running test-statistics could also help in analyzing the data further.

* Important data acquisition ( either by client, business teams or an external API) and rigorous feature engineering.

* Better way to fill the missing values in columns, based on the distribution of the data column

* Better way to handle outliers. In case of large number of records people use Auto-encoders as well.

* Time series based anomaly detection in the dataset and then modeling it appropriately
"""